{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doubleml as dml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from flaml import AutoML\n",
    "from xgboost import XGBRegressor\n",
    "from doubleml._utils_resampling import DoubleMLResampling\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from ..dgp.make_BCH import DGP_BCH2014\n",
    "import matplotlib.pyplot as plt\n",
    "from ..doubleml_flaml_api.doubleml_flaml_api import FlamlRegressorDoubleML, FlamlClassifierDoubleML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## with ACIC data\n",
    "n_folds = 4\n",
    "no_iter = 100\n",
    "\n",
    "for n_obs in [20,50,100,250,500,1000,2000,5000,10000,200000]:\n",
    "    res_path = f\"../results/bch/XGBoost/\"\n",
    "    res_fullsample = []\n",
    "\n",
    "    for k in range(no_iter):\n",
    "        # for saving each iteration\n",
    "        _res_full = []\n",
    "        \n",
    "        np.random.seed(k)\n",
    "        \n",
    "        # load data\n",
    "        X,y,d = DGP_BCH2014(theta=0.5, n_obs=n_obs, dim_x=200)  \n",
    "        x_cols = [f'X{i + 1}' for i in np.arange(X.shape[1])]\n",
    "        df = pd.DataFrame(np.column_stack((X, y, d)),\n",
    "                            columns=x_cols + ['y', 'd'])      \n",
    "\n",
    "        # full sample, tuning\n",
    "        ml_l = XGBRegressor()\n",
    "        ml_m = XGBRegressor()\n",
    "        \n",
    "        # full sample, doubleml\n",
    "        np.random.seed(k)\n",
    "        obj_dml_data = dml.DoubleMLData(df,y_col='y',d_cols='d')\n",
    "        dml_plr_automl = dml.DoubleMLPLR(obj_dml_data, ml_l = ml_l, ml_m = ml_m, n_folds = n_folds)\n",
    "        dml_plr_automl.fit(store_predictions=True)\n",
    "\n",
    "        _res_full.append(dml_plr_automl.summary[\"coef\"].values[0])\n",
    "        _res_full.append(dml_plr_automl.summary[\"2.5 %\"].values[0])\n",
    "        _res_full.append(dml_plr_automl.summary[\"97.5 %\"].values[0])\n",
    "        _res_full.append(mean_squared_error(y, dml_plr_automl.summary[\"coef\"].values[0] * d + dml_plr_automl.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_full.append(mean_squared_error(y, dml_plr_automl.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_full.append(mean_squared_error(d, dml_plr_automl.predictions[\"ml_m\"][:,0,0]))\n",
    "\n",
    "        # add this iteration to overall results\n",
    "        res_fullsample.append(_res_full)\n",
    "\n",
    "        # save current result\n",
    "        pd.DataFrame(res_fullsample, columns = [\"coef\",\"2.5%\",\"97.5%\",\"loss_Y\",\"fs_loss_mll\",\"fs_loss_mlm\"]).to_csv(res_path + f\"{n_obs}_default.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_l = {'max_depth': [4,5,6],\n",
    "        'n_estimators': [100],\n",
    "        'n_jobs': [-1]}\n",
    "grid_m = {'max_depth': [4,5,6],\n",
    "        'n_estimators': [100],\n",
    "        'n_jobs': [-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## with ACIC data\n",
    "n_folds = 4\n",
    "no_iter = 100\n",
    "\n",
    "for n_obs in [10,50,100,250,500,1000,2000,5000]:\n",
    "    res_path = f\"../results/bch/RandomForest/\"\n",
    "    res_fullsample = []\n",
    "    res_splitsample = []\n",
    "    res_onfolds = []\n",
    "    for k in range(no_iter):\n",
    "        # for saving each iteration\n",
    "        _res_full = []\n",
    "        _res_split = []\n",
    "        _res_of = []\n",
    "        \n",
    "        np.random.seed(k)\n",
    "\n",
    "        # load data\n",
    "        X,y,d = DGP_BCH2014(theta=0.5, n_obs=n_obs, dim_x=200)  \n",
    "        x_cols = [f'X{i + 1}' for i in np.arange(X.shape[1])]\n",
    "        df = pd.DataFrame(np.column_stack((X, y, d)),\n",
    "                            columns=x_cols + ['y', 'd'])  \n",
    "    \n",
    "        # full sample, tuning\n",
    "        gs_l = GridSearchCV(RandomForestRegressor(), grid_l, n_jobs=-1, scoring=\"neg_mean_squared_error\")\n",
    "        gs_l.fit(X,y)\n",
    "        gs_m = GridSearchCV(RandomForestRegressor(), grid_m, n_jobs=-1, scoring=\"neg_mean_squared_error\")\n",
    "        gs_m.fit(X,d)\n",
    "\n",
    "        ml_l = gs_l.best_estimator_\n",
    "        ml_m = gs_m.best_estimator_\n",
    "\n",
    "        # full sample, doubleml\n",
    "        obj_dml_data = dml.DoubleMLData(df,y_col='y',d_cols='d')\n",
    "        dml_plr = dml.DoubleMLPLR(obj_dml_data, ml_l = ml_l, ml_m = ml_m, n_folds = n_folds)\n",
    "        dml_plr.fit(store_predictions=True)\n",
    "\n",
    "        _res_full.append(dml_plr.summary[\"coef\"].values[0])\n",
    "        _res_full.append(dml_plr.summary[\"2.5 %\"].values[0])\n",
    "        _res_full.append(dml_plr.summary[\"97.5 %\"].values[0])\n",
    "        _res_full.append(-1 * gs_l.best_score_)\n",
    "        _res_full.append(-1 * gs_m.best_score_)\n",
    "        _res_full.append(mean_squared_error(y, dml_plr.summary[\"coef\"].values[0] * d + dml_plr.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_full.append(mean_squared_error(y, dml_plr.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_full.append(mean_squared_error(d, dml_plr.predictions[\"ml_m\"][:,0,0]))\n",
    "\n",
    "        # split sample, tuning\n",
    "        df_tune, df_test = train_test_split(df, test_size= 0.5, random_state = 42*k)\n",
    "        y_tune, d_tune, X_tune = df_tune[\"y\"], df_tune[\"d\"], df_tune.drop(columns=[\"y\",\"d\"])\n",
    "        \n",
    "        gs_l = GridSearchCV(RandomForestRegressor(), grid_l, n_jobs=-1, scoring=\"neg_mean_squared_error\")\n",
    "        gs_l.fit(X_tune,y_tune)\n",
    "        gs_m = GridSearchCV(RandomForestRegressor(), grid_m, n_jobs=-1, scoring=\"neg_mean_squared_error\")\n",
    "        gs_m.fit(X_tune,d_tune)\n",
    "\n",
    "        ml_l = gs_l.best_estimator_\n",
    "        ml_m = gs_m.best_estimator_\n",
    "\n",
    "        # split sample, doubleml           \n",
    "        obj_dml_data = dml.DoubleMLData(df_test, y_col='y', d_cols='d')\n",
    "        dml_plr_split = dml.DoubleMLPLR(obj_dml_data, ml_l = ml_l, ml_m = ml_m, n_folds = n_folds)\n",
    "        dml_plr_split.fit(store_predictions = True)\n",
    "\n",
    "        _res_split.append(dml_plr_split.summary[\"coef\"].values[0])\n",
    "        _res_split.append(dml_plr_split.summary[\"2.5 %\"].values[0])\n",
    "        _res_split.append(dml_plr_split.summary[\"97.5 %\"].values[0])\n",
    "        _res_split.append(-1 * gs_l.best_score_)\n",
    "        _res_split.append(-1 * gs_m.best_score_)\n",
    "        _res_split.append(mean_squared_error(df_test[\"y\"], dml_plr_split.summary[\"coef\"].values[0] * df_test[\"d\"] +  dml_plr_split.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_split.append(mean_squared_error(df_test[\"y\"], dml_plr_split.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_split.append(mean_squared_error(df_test[\"d\"], dml_plr_split.predictions[\"ml_m\"][:,0,0]))\n",
    "\n",
    "        # on folds\n",
    "        ml_l = RandomForestRegressor()\n",
    "        ml_m = RandomForestRegressor()\n",
    "\n",
    "        obj_dml_data = dml.DoubleMLData(df,y_col='y',d_cols='d')\n",
    "\n",
    "        dml_plr_onfolds = dml.DoubleMLPLR(obj_dml_data, ml_l = ml_l, ml_m = ml_m, n_folds = n_folds)\n",
    "        dml_plr_onfolds.tune({\"ml_l\": grid_l, \"ml_m\": grid_m}, tune_on_folds=True)\n",
    "        dml_plr_onfolds.fit(store_predictions=True, store_models = True)\n",
    "\n",
    "        _res_of.append(dml_plr_onfolds.summary[\"coef\"].values[0])\n",
    "        _res_of.append(dml_plr_onfolds.summary[\"2.5 %\"].values[0])\n",
    "        _res_of.append(dml_plr_onfolds.summary[\"97.5 %\"].values[0])\n",
    "        _res_of.append(np.nan)\n",
    "        _res_of.append(np.nan)\n",
    "        _res_of.append(mean_squared_error(y, dml_plr_onfolds.summary[\"coef\"].values[0] * d + dml_plr_onfolds.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_of.append(mean_squared_error(y, dml_plr_onfolds.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_of.append(mean_squared_error(d, dml_plr_onfolds.predictions[\"ml_m\"][:,0,0]))\n",
    "        \n",
    "        # # add this iteration to overall results\n",
    "        res_fullsample.append(_res_full)\n",
    "        res_splitsample.append(_res_split)\n",
    "        res_onfolds.append(_res_of)\n",
    "\n",
    "        # save current result\n",
    "        pd.DataFrame(res_fullsample, columns = [\"coef\",\"2.5%\",\"97.5%\",\"tune_loss_mll\",\"tune_loss_mlm\",\"loss_Y\",\"fs_loss_mll\",\"fs_loss_mlm\"]).to_csv(res_path + f\"{n_obs}_fullsample.csv\")\n",
    "        pd.DataFrame(res_splitsample, columns=[\"coef\",\"2.5%\",\"97.5%\",\"tune_loss_mll\",\"tune_loss_mlm\",\"loss_Y\",\"fs_loss_mll\",\"fs_loss_mlm\"]).to_csv(res_path + f\"{n_obs}_splitsample.csv\")\n",
    "        pd.DataFrame(res_onfolds, columns=[\"coef\",\"2.5%\",\"97.5%\",\"tune_loss_mll\",\"tune_loss_mlm\",\"loss_Y\",\"fs_loss_mll\",\"fs_loss_mlm\"]).to_csv(res_path + f\"{n_obs}_onfolds.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## with ACIC data\n",
    "n_folds = 4\n",
    "no_iter = 100\n",
    "t = 60\n",
    "\n",
    "for n_obs in [10,50,100,250,500,1000,2000,5000,10000,200000]:\n",
    "    res_path = f\"simulations/results/bch/FLAML/\"\n",
    "    res_fullsample = []\n",
    "    res_splitsample = []\n",
    "    res_onfolds = []\n",
    "    for k in range(no_iter):\n",
    "        # for saving each iteration\n",
    "        _res_full = []\n",
    "        _res_split = []\n",
    "        _res_of = []\n",
    "        \n",
    "        # load data\n",
    "        X,y,d = DGP_BCH2014(theta=0.5, n_obs=n_obs, dim_x=200)  \n",
    "        x_cols = [f'X{i + 1}' for i in np.arange(X.shape[1])]\n",
    "        df = pd.DataFrame(np.column_stack((X, y, d)),\n",
    "                        columns=x_cols + ['y', 'd'])  \n",
    "    \n",
    "        # full sample, tuning\n",
    "        while True:\n",
    "            try:\n",
    "                automl_y = AutoML()\n",
    "                automl_y.fit(X, y, task=\"regression\", time_budget=t, metric=\"mse\", verbose=False, estimator_list = None)\n",
    "                automl_d = AutoML()\n",
    "                automl_d.fit(X, d, task=\"regression\", time_budget=t, metric=\"mse\", verbose=False, estimator_list = None)\n",
    "\n",
    "                ml_l = automl_y.model.estimator\n",
    "                ml_m = automl_d.model.estimator\n",
    "                break\n",
    "            except AttributeError: \n",
    "                pass\n",
    "\n",
    "        # full sample, doubleml\n",
    "        np.random.seed(k)\n",
    "        obj_dml_data = dml.DoubleMLData(df,y_col='y',d_cols='d')\n",
    "        dml_plr_automl = dml.DoubleMLPLR(obj_dml_data, ml_l = ml_l, ml_m = ml_m, n_folds = n_folds)\n",
    "        dml_plr_automl.fit(store_predictions=True)\n",
    "\n",
    "        _res_full.append(dml_plr_automl.summary[\"coef\"].values[0])\n",
    "        _res_full.append(dml_plr_automl.summary[\"2.5 %\"].values[0])\n",
    "        _res_full.append(dml_plr_automl.summary[\"97.5 %\"].values[0])\n",
    "        _res_full.append(automl_y.best_loss)\n",
    "        _res_full.append(automl_d.best_loss)\n",
    "        _res_full.append(mean_squared_error(y, dml_plr_automl.summary[\"coef\"].values[0] * d + dml_plr_automl.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_full.append(mean_squared_error(y, dml_plr_automl.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_full.append(mean_squared_error(d, dml_plr_automl.predictions[\"ml_m\"][:,0,0]))\n",
    "\n",
    "        # split sample, tuning\n",
    "        \n",
    "        df_tune, df_test = train_test_split(df, test_size= 0.5, random_state = 42*k)\n",
    "        y_tune, d_tune, X_tune = df_tune[\"y\"], df_tune[\"d\"], df_tune.drop(columns=[\"y\",\"d\"])\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                automl_y = AutoML()\n",
    "                automl_y.fit(X_tune, y_tune, task=\"regression\", time_budget=t, metric= \"mse\", verbose=False, estimator_list = None)\n",
    "                automl_d = AutoML()\n",
    "                automl_d.fit(X_tune, d_tune, task=\"regression\", time_budget=t, metric= \"mse\", verbose=False, estimator_list = None)\n",
    "\n",
    "                ml_l = automl_y.model.estimator\n",
    "                ml_m = automl_d.model.estimator\n",
    "                break \n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "        # split sample, doubleml           \n",
    "        np.random.seed(2*k)\n",
    "        obj_dml_data = dml.DoubleMLData(df_test, y_col='y', d_cols='d')\n",
    "        dml_plr_automl_split = dml.DoubleMLPLR(obj_dml_data, ml_l = ml_l, ml_m = ml_m, n_folds = n_folds)\n",
    "        dml_plr_automl_split.fit(store_predictions = True)\n",
    "\n",
    "        _res_split.append(dml_plr_automl_split.summary[\"coef\"].values[0])\n",
    "        _res_split.append(dml_plr_automl_split.summary[\"2.5 %\"].values[0])\n",
    "        _res_split.append(dml_plr_automl_split.summary[\"97.5 %\"].values[0])\n",
    "        _res_split.append(automl_y.best_loss)\n",
    "        _res_split.append(automl_d.best_loss)\n",
    "        _res_split.append(mean_squared_error(df_test[\"y\"], dml_plr_automl_split.summary[\"coef\"].values[0] * df_test[\"d\"] + dml_plr_automl_split.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_split.append(mean_squared_error(df_test[\"y\"], dml_plr_automl_split.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_split.append(mean_squared_error(df_test[\"d\"], dml_plr_automl_split.predictions[\"ml_m\"][:,0,0]))\n",
    "\n",
    "        # on folds\n",
    "        while True:\n",
    "            try:\n",
    "                ml_l = FlamlRegressorDoubleML(time = (t/4), metric=\"mse\", estimator_list = None)\n",
    "                ml_m = FlamlRegressorDoubleML(time = (t/4), metric=\"mse\", estimator_list = None)\n",
    "\n",
    "                obj_dml_data = dml.DoubleMLData(df,y_col='y',d_cols='d')\n",
    "        \n",
    "                np.random.seed(3*k)\n",
    "                dml_plr_automl_onfolds = dml.DoubleMLPLR(obj_dml_data, ml_l = ml_l, ml_m = ml_m, n_folds = n_folds)\n",
    "                dml_plr_automl_onfolds.fit(store_predictions=True, store_models = True)\n",
    "                break\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "        _res_of.append(dml_plr_automl_onfolds.summary[\"coef\"].values[0])\n",
    "        _res_of.append(dml_plr_automl_onfolds.summary[\"2.5 %\"].values[0])\n",
    "        _res_of.append(dml_plr_automl_onfolds.summary[\"97.5 %\"].values[0])\n",
    "        _res_of.append(np.mean([i.auto_ml.best_loss for i in dml_plr_automl_onfolds.models['ml_l'][\"d\"][0]]))\n",
    "        _res_of.append(np.mean([i.auto_ml.best_loss for i in dml_plr_automl_onfolds.models['ml_m'][\"d\"][0]]))\n",
    "        _res_of.append(mean_squared_error(y, dml_plr_automl_onfolds.summary[\"coef\"].values[0] * d + dml_plr_automl_onfolds.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_of.append(mean_squared_error(y,  dml_plr_automl_onfolds.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_of.append(mean_squared_error(d, dml_plr_automl_onfolds.predictions[\"ml_m\"][:,0,0]))\n",
    "        \n",
    "        # add this iteration to overall results\n",
    "        res_fullsample.append(_res_full)\n",
    "        res_splitsample.append(_res_split)\n",
    "        res_onfolds.append(_res_of)\n",
    "\n",
    "        # save current result\n",
    "        pd.DataFrame(res_fullsample, columns = [\"coef\",\"2.5%\",\"97.5%\",\"tune_loss_mll\",\"tune_loss_mlm\",\"loss_Y\",\"fs_loss_mll\",\"fs_loss_mlm\"]).to_csv(res_path + f\"{n_obs}_fullsample.csv\")\n",
    "        pd.DataFrame(res_splitsample, columns=[\"coef\",\"2.5%\",\"97.5%\",\"tune_loss_mll\",\"tune_loss_mlm\",\"loss_Y\",\"fs_loss_mll\",\"fs_loss_mlm\"]).to_csv(res_path + f\"{n_obs}_splitsample.csv\")\n",
    "        pd.DataFrame(res_onfolds, columns=[\"coef\",\"2.5%\",\"97.5%\",\"tune_loss_mll\",\"tune_loss_mlm\",\"loss_Y\",\"fs_loss_mll\",\"fs_loss_mlm\"]).to_csv(res_path + f\"{n_obs}_onfolds.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml05",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
