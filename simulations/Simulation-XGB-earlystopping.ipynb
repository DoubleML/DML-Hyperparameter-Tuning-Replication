{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import doubleml as dml\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from doubleml._utils_resampling import DoubleMLResampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## with ACIC data\n",
    "n_folds = 4\n",
    "no_iter = 100\n",
    "\n",
    "for sc in range(16):\n",
    "    sc+=1\n",
    "    res_path = f\"../results/acic/plr/XGBoost/Scenario{sc}_\"\n",
    "    res_fullsample = []\n",
    "    res_splitsample = []\n",
    "    res_onfolds = []\n",
    "    for k in range(no_iter):\n",
    "        #print(k)\n",
    "        # for saving each iteration\n",
    "        _res_full = []\n",
    "        _res_split = []\n",
    "        _res_of = []\n",
    "        \n",
    "        # load data\n",
    "        df = pd.read_csv(f\"..dgp/acic/Scenario{sc}/CHDScenario{sc}DS{k+1}.csv\")           \n",
    "        y, d, X = df[\"Y\"].values, df[\"A\"].values, df.drop(columns=[\"Y\",\"A\"]).values\n",
    "    \n",
    "        # full sample, tuning\n",
    "        n_trees_ml_m, n_trees_ml_l = 0, 0  \n",
    "        kf = KFold(n_splits=n_folds)\n",
    "        for _, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "            tune_l = XGBRegressor(n_estimators=100, max_depth=2, learning_rate=0.1, early_stopping_rounds=8, eval_metric=\"rmse\")\n",
    "            tune_l.fit(X[train_index], y[train_index], eval_set=[(X[test_index], y[test_index])], verbose=False)\n",
    "            tune_m = XGBClassifier(n_estimators=100, max_depth=2, learning_rate=0.1, early_stopping_rounds=8, eval_metric=\"logloss\")\n",
    "            tune_m.fit(X[train_index], d[train_index], eval_set=[(X[test_index], d[test_index])], verbose=False)\n",
    "            n_trees_ml_m += tune_m.best_ntree_limit\n",
    "            n_trees_ml_l += tune_l.best_ntree_limit\n",
    "\n",
    "        ml_l = XGBRegressor(n_estimators= int(n_trees_ml_l / n_folds), max_depth=2, learning_rate=0.1)\n",
    "        ml_m = XGBClassifier(n_estimators= int(n_trees_ml_m / n_folds), max_depth=2, learning_rate=0.1)\n",
    "\n",
    "        # full sample, doubleml\n",
    "        np.random.seed(k)\n",
    "        obj_dml_data = dml.DoubleMLData(df,y_col='Y',d_cols='A')\n",
    "        dml_plr = dml.DoubleMLPLR(obj_dml_data, ml_l = ml_l, ml_m = ml_m, n_folds = n_folds)\n",
    "        dml_plr.fit(store_predictions=True)\n",
    "\n",
    "        _res_full.append(dml_plr.summary[\"coef\"].values[0])\n",
    "        _res_full.append(dml_plr.summary[\"2.5 %\"].values[0])\n",
    "        _res_full.append(dml_plr.summary[\"97.5 %\"].values[0])\n",
    "        _res_full.append(np.sqrt(tune_l.best_score))\n",
    "        _res_full.append(tune_m.best_score)\n",
    "        _res_full.append(mean_squared_error(y, dml_plr.summary[\"coef\"].values[0] * d + dml_plr.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_full.append(mean_squared_error(y, dml_plr.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_full.append(log_loss(d, dml_plr.predictions[\"ml_m\"][:,0,0]))\n",
    "\n",
    "        # split sample, tuning\n",
    "        df_tune, df_test = train_test_split(df, test_size= 0.5, random_state = 42*k)\n",
    "        y_tune, d_tune, X_tune = df_tune[\"Y\"].values, df_tune[\"A\"].values, df_tune.drop(columns=[\"Y\",\"A\"]).values\n",
    "        y_test, d_test, X_test = df_test[\"Y\"].values, df_test[\"A\"].values, df_test.drop(columns=[\"Y\",\"A\"]).values\n",
    "        \n",
    "        tune_l = XGBRegressor(n_estimators=100, max_depth=2, learning_rate=0.1, early_stopping_rounds=8, eval_metric=\"rmse\")\n",
    "        tune_l.fit(X_tune, y_tune, eval_set=[(X_test, y_test)], verbose=False)\n",
    "        tune_m = XGBClassifier(n_estimators=100, max_depth=2, learning_rate=0.1, early_stopping_rounds=8, eval_metric=\"logloss\")\n",
    "        tune_m.fit(X_tune, d_tune, eval_set=[(X_test, d_test)], verbose=False)\n",
    "\n",
    "        ml_l = XGBRegressor(n_estimators= tune_l.best_ntree_limit, max_depth=2, learning_rate=0.1)\n",
    "        ml_m = XGBClassifier(n_estimators= tune_m.best_ntree_limit, max_depth=2, learning_rate=0.1)\n",
    "\n",
    "        # split sample, doubleml           \n",
    "        np.random.seed(2*k)\n",
    "        obj_dml_data = dml.DoubleMLData(df_test, y_col='Y', d_cols='A')\n",
    "        dml_plr_split = dml.DoubleMLPLR(obj_dml_data, ml_l = ml_l, ml_m = ml_m, n_folds = n_folds)\n",
    "        dml_plr_split.fit(store_predictions = True)\n",
    "\n",
    "        _res_split.append(dml_plr_split.summary[\"coef\"].values[0])\n",
    "        _res_split.append(dml_plr_split.summary[\"2.5 %\"].values[0])\n",
    "        _res_split.append(dml_plr_split.summary[\"97.5 %\"].values[0])\n",
    "        _res_split.append(np.sqrt(tune_l.best_score))\n",
    "        _res_split.append(tune_m.best_score)\n",
    "        _res_split.append(mean_squared_error(y_test, dml_plr_split.summary[\"coef\"].values[0] * d_test + dml_plr_split.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_split.append(mean_squared_error(df_test[\"Y\"], dml_plr_split.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_split.append(log_loss(df_test[\"A\"], dml_plr_split.predictions[\"ml_m\"][:,0,0]))\n",
    "\n",
    "        # on folds\n",
    "        smpls = DoubleMLResampling(n_folds=n_folds, n_rep=1, n_obs=y.shape[0], apply_cross_fitting=True).split_samples()\n",
    "        param_list_ml_l = []\n",
    "        param_list_ml_m = []\n",
    "        for _, (train_index, test_index) in enumerate(smpls[0]):\n",
    "            tune_l = XGBRegressor(n_estimators=100, max_depth=2, learning_rate=0.1, early_stopping_rounds=8, eval_metric=\"rmse\")\n",
    "            tune_l.fit(X[train_index], y[train_index], eval_set=[(X[test_index], y[test_index])], verbose=False)\n",
    "            tune_m = XGBClassifier(n_estimators=100, max_depth=2, learning_rate=0.1, early_stopping_rounds=8, eval_metric=\"logloss\")\n",
    "            tune_m.fit(X[train_index], d[train_index], eval_set=[(X[test_index], d[test_index])], verbose=False)\n",
    "            param_list_ml_l.append({'n_estimators': tune_l.best_ntree_limit, 'max_depth': 2, 'learning_rate': 0.1})\n",
    "            param_list_ml_m.append({'n_estimators': tune_m.best_ntree_limit, 'max_depth': 2, 'learning_rate': 0.1})\n",
    "        param_list_ml_l = [param_list_ml_l]\n",
    "        param_list_ml_m = [param_list_ml_m]\n",
    "\n",
    "        obj_dml_data = dml.DoubleMLData(df,y_col='Y',d_cols='A')\n",
    "        ml_l = XGBRegressor()\n",
    "        ml_m = XGBClassifier()\n",
    "\n",
    "        np.random.seed(3*k)\n",
    "        dml_plr_onfolds = dml.DoubleMLPLR(obj_dml_data, ml_l = ml_l, ml_m = ml_m, n_folds = n_folds, draw_sample_splitting=False)\n",
    "        dml_plr_onfolds.set_sample_splitting(smpls)\n",
    "        dml_plr_onfolds.set_ml_nuisance_params(\"ml_l\", \"A\", param_list_ml_l)\n",
    "        dml_plr_onfolds.set_ml_nuisance_params(\"ml_m\", \"A\", param_list_ml_m)\n",
    "        dml_plr_onfolds.fit(store_predictions=True, store_models = True)\n",
    "\n",
    "        _res_of.append(dml_plr_onfolds.summary[\"coef\"].values[0])\n",
    "        _res_of.append(dml_plr_onfolds.summary[\"2.5 %\"].values[0])\n",
    "        _res_of.append(dml_plr_onfolds.summary[\"97.5 %\"].values[0])\n",
    "        _res_of.append(np.nan)\n",
    "        _res_of.append(np.nan)\n",
    "        _res_of.append(mean_squared_error(y, dml_plr_onfolds.summary[\"coef\"].values[0] * d + dml_plr_onfolds.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_of.append(mean_squared_error(y, dml_plr_onfolds.predictions[\"ml_l\"][:,0,0]))\n",
    "        _res_of.append(log_loss(d, dml_plr_onfolds.predictions[\"ml_m\"][:,0,0]))\n",
    "        \n",
    "        # # add this iteration to overall results\n",
    "        res_fullsample.append(_res_full)\n",
    "        res_splitsample.append(_res_split)\n",
    "        res_onfolds.append(_res_of)\n",
    "\n",
    "        # save current result\n",
    "        pd.DataFrame(res_fullsample, columns = [\"coef\",\"2.5%\",\"97.5%\",\"tune_loss_mll\",\"tune_loss_mlm\",\"loss_Y\",\"fs_loss_mll\",\"fs_loss_mlm\"]).to_csv(res_path + f\"fullsample.csv\")\n",
    "        pd.DataFrame(res_splitsample, columns=[\"coef\",\"2.5%\",\"97.5%\",\"tune_loss_mll\",\"tune_loss_mlm\",\"loss_Y\",\"fs_loss_mll\",\"fs_loss_mlm\"]).to_csv(res_path + f\"splitsample.csv\")\n",
    "        pd.DataFrame(res_onfolds, columns=[\"coef\",\"2.5%\",\"97.5%\",\"tune_loss_mll\",\"tune_loss_mlm\",\"loss_Y\",\"fs_loss_mll\",\"fs_loss_mlm\"]).to_csv(res_path + f\"onfolds.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## with ACIC data\n",
    "n_folds = 4\n",
    "no_iter = 100\n",
    "\n",
    "for sc in range(16):\n",
    "    sc+=1\n",
    "    res_path = f\"../results/acic/irm/XGBoost/Scenario{sc}_\"\n",
    "    res_fullsample = []\n",
    "    res_splitsample = []\n",
    "    res_onfolds = []\n",
    "    for k in range(no_iter):\n",
    "        #print(k)\n",
    "        # for saving each iteration\n",
    "        _res_full = []\n",
    "        _res_split = []\n",
    "        _res_of = []\n",
    "        \n",
    "        # load data\n",
    "        df = pd.read_csv(f\"../dgp/acic/Scenario{sc}/CHDScenario{sc}DS{k+1}.csv\")           \n",
    "        y, d, X = df[\"Y\"].values, df[\"A\"].values, df.drop(columns=[\"Y\",\"A\"]).values\n",
    "    \n",
    "        # full sample, tuning\n",
    "        n_trees_ml_m, n_trees_ml_g = 0, 0  \n",
    "        kf = KFold(n_splits=n_folds)\n",
    "        for _, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "            tune_g = XGBRegressor(n_estimators=100, max_depth=2, learning_rate=0.1, early_stopping_rounds=8, eval_metric=\"rmse\")\n",
    "            tune_g.fit(np.c_[X,d][train_index], y[train_index], eval_set=[(np.c_[X,d][test_index], y[test_index])], verbose=False)\n",
    "            tune_m = XGBClassifier(n_estimators=100, max_depth=2, learning_rate=0.1, early_stopping_rounds=8, eval_metric=\"logloss\")\n",
    "            tune_m.fit(X[train_index], d[train_index], eval_set=[(X[test_index], d[test_index])], verbose=False)\n",
    "            n_trees_ml_m += tune_m.best_ntree_limit\n",
    "            n_trees_ml_g += tune_g.best_ntree_limit\n",
    "\n",
    "        ml_g = XGBRegressor(n_estimators= int(n_trees_ml_g / n_folds), max_depth=2, learning_rate=0.1)\n",
    "        ml_m = XGBClassifier(n_estimators= int(n_trees_ml_m / n_folds), max_depth=2, learning_rate=0.1)\n",
    "\n",
    "        # full sample, doubleml\n",
    "        np.random.seed(k)\n",
    "        obj_dml_data = dml.DoubleMLData(df,y_col='Y',d_cols='A')\n",
    "        dml_irm = dml.DoubleMLIRM(obj_dml_data, ml_g = ml_g, ml_m = ml_m, n_folds = n_folds)\n",
    "        dml_irm.fit(store_predictions=True)\n",
    "\n",
    "        _res_full.append(dml_irm.summary[\"coef\"].values[0])\n",
    "        _res_full.append(dml_irm.summary[\"2.5 %\"].values[0])\n",
    "        _res_full.append(dml_irm.summary[\"97.5 %\"].values[0])\n",
    "        _res_full.append(np.sqrt(tune_g.best_score))\n",
    "        _res_full.append(tune_m.best_score)\n",
    "        treat_ind = (df[\"A\"] == 1)\n",
    "        ml_g_pred = treat_ind * dml_irm.predictions[\"ml_g1\"][:,0,0] + (1 - treat_ind) * dml_irm.predictions[\"ml_g0\"][:,0,0]\n",
    "        _res_full.append(mean_squared_error(y, ml_g_pred))\n",
    "        _res_full.append(log_loss(d, dml_irm.predictions[\"ml_m\"][:,0,0]))\n",
    "\n",
    "        # split sample, tuning\n",
    "        df_tune, df_test = train_test_split(df, test_size= 0.5, random_state = 42*k)\n",
    "        y_tune, d_tune, X_tune = df_tune[\"Y\"].values, df_tune[\"A\"].values, df_tune.drop(columns=[\"Y\",\"A\"]).values\n",
    "        y_test, d_test, X_test = df_test[\"Y\"].values, df_test[\"A\"].values, df_test.drop(columns=[\"Y\",\"A\"]).values\n",
    "        \n",
    "        tune_g = XGBRegressor(n_estimators=100, max_depth=2, learning_rate=0.1, early_stopping_rounds=8, eval_metric=\"rmse\")\n",
    "        tune_g.fit(np.c_[X_tune, d_tune], y_tune, eval_set=[(np.c_[X_test, d_test], y_test)], verbose=False)\n",
    "        tune_m = XGBClassifier(n_estimators=100, max_depth=2, learning_rate=0.1, early_stopping_rounds=8, eval_metric=\"logloss\")\n",
    "        tune_m.fit(X_tune, d_tune, eval_set=[(X_test, d_test)], verbose=False)\n",
    "\n",
    "        ml_g = XGBRegressor(n_estimators= tune_g.best_ntree_limit, max_depth=2, learning_rate=0.1)\n",
    "        ml_m = XGBClassifier(n_estimators= tune_m.best_ntree_limit, max_depth=2, learning_rate=0.1)\n",
    "\n",
    "        # split sample, doubleml           \n",
    "        np.random.seed(2*k)\n",
    "        obj_dml_data = dml.DoubleMLData(df_test, y_col='Y', d_cols='A')\n",
    "        dml_irm_split = dml.DoubleMLIRM(obj_dml_data, ml_g = ml_g, ml_m = ml_m, n_folds = n_folds)\n",
    "        dml_irm_split.fit(store_predictions = True)\n",
    "\n",
    "        _res_split.append(dml_irm_split.summary[\"coef\"].values[0])\n",
    "        _res_split.append(dml_irm_split.summary[\"2.5 %\"].values[0])\n",
    "        _res_split.append(dml_irm_split.summary[\"97.5 %\"].values[0])\n",
    "        _res_split.append(np.sqrt(tune_g.best_score))\n",
    "        _res_split.append(tune_m.best_score)\n",
    "        treat_ind = (df_test[\"A\"] == 1)\n",
    "        ml_g_pred = treat_ind * dml_irm_split.predictions[\"ml_g1\"][:,0,0] + (1 - treat_ind) * dml_irm_split.predictions[\"ml_g0\"][:,0,0]\n",
    "        _res_split.append(mean_squared_error(df_test[\"Y\"], ml_g_pred))\n",
    "        _res_split.append(log_loss(df_test[\"A\"], dml_irm_split.predictions[\"ml_m\"][:,0,0]))\n",
    "\n",
    "        # on folds\n",
    "        smpls = DoubleMLResampling(n_folds=n_folds, n_rep=1, n_obs=y.shape[0], apply_cross_fitting=True).split_samples()\n",
    "        param_list_ml_g = []\n",
    "        param_list_ml_m = []\n",
    "        for _, (train_index, test_index) in enumerate(smpls[0]):\n",
    "            tune_g = XGBRegressor(n_estimators=100, max_depth=2, learning_rate=0.1, early_stopping_rounds=8, eval_metric=\"rmse\")\n",
    "            tune_g.fit(np.c_[X,d][train_index], y[train_index], eval_set=[(np.c_[X,d][test_index], y[test_index])], verbose=False)\n",
    "            tune_m = XGBClassifier(n_estimators=100, max_depth=2, learning_rate=0.1, early_stopping_rounds=8, eval_metric=\"logloss\")\n",
    "            tune_m.fit(X[train_index], d[train_index], eval_set=[(X[test_index], d[test_index])], verbose=False)\n",
    "            param_list_ml_g.append({'n_estimators': tune_g.best_ntree_limit, 'max_depth': 2, 'learning_rate': 0.1})\n",
    "            param_list_ml_m.append({'n_estimators': tune_m.best_ntree_limit, 'max_depth': 2, 'learning_rate': 0.1})\n",
    "        param_list_ml_g = [param_list_ml_g]\n",
    "        param_list_ml_m = [param_list_ml_m]\n",
    "\n",
    "        obj_dml_data = dml.DoubleMLData(df,y_col='Y',d_cols='A')\n",
    "\n",
    "        ml_g = XGBRegressor()\n",
    "        ml_m = XGBClassifier()\n",
    "\n",
    "        np.random.seed(3*k)\n",
    "        dml_irm_onfolds = dml.DoubleMLIRM(obj_dml_data, ml_g = ml_g, ml_m = ml_m, n_folds = n_folds, draw_sample_splitting=False)\n",
    "        dml_irm_onfolds.set_sample_splitting(smpls)\n",
    "        dml_irm_onfolds.set_ml_nuisance_params(\"ml_g0\", \"A\", param_list_ml_g)\n",
    "        dml_irm_onfolds.set_ml_nuisance_params(\"ml_g1\", \"A\", param_list_ml_g)\n",
    "        dml_irm_onfolds.set_ml_nuisance_params(\"ml_m\", \"A\", param_list_ml_m)\n",
    "        dml_irm_onfolds.fit(store_predictions=True, store_models = True)\n",
    "\n",
    "        _res_of.append(dml_irm_onfolds.summary[\"coef\"].values[0])\n",
    "        _res_of.append(dml_irm_onfolds.summary[\"2.5 %\"].values[0])\n",
    "        _res_of.append(dml_irm_onfolds.summary[\"97.5 %\"].values[0])\n",
    "        _res_of.append(np.nan)\n",
    "        _res_of.append(np.nan)\n",
    "        treat_ind = (df[\"A\"] == 1)\n",
    "        ml_g_pred = treat_ind * dml_irm_onfolds.predictions[\"ml_g1\"][:,0,0] + (1 - treat_ind) * dml_irm_onfolds.predictions[\"ml_g0\"][:,0,0]\n",
    "        _res_of.append(mean_squared_error(y, ml_g_pred))\n",
    "        _res_of.append(log_loss(d, dml_irm_onfolds.predictions[\"ml_m\"][:,0,0]))\n",
    "        \n",
    "        # # add this iteration to overall results\n",
    "        res_fullsample.append(_res_full)\n",
    "        res_splitsample.append(_res_split)\n",
    "        res_onfolds.append(_res_of)\n",
    "\n",
    "        # save current result\n",
    "        pd.DataFrame(res_fullsample, columns = [\"coef\",\"2.5%\",\"97.5%\",\"tune_loss_mlg\",\"tune_loss_mlm\",\"fs_loss_mlg\",\"fs_loss_mlm\"]).to_csv(res_path + f\"fullsample.csv\")\n",
    "        pd.DataFrame(res_splitsample, columns=[\"coef\",\"2.5%\",\"97.5%\",\"tune_loss_mlg\",\"tune_loss_mlm\",\"fs_loss_mlg\",\"fs_loss_mlm\"]).to_csv(res_path + f\"splitsample.csv\")\n",
    "        pd.DataFrame(res_onfolds, columns=[\"coef\",\"2.5%\",\"97.5%\",\"tune_loss_mlg\",\"tune_loss_mlm\",\"fs_loss_mlg\",\"fs_loss_mlm\"]).to_csv(res_path + f\"onfolds.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
